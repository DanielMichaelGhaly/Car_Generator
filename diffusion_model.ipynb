{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Used the StanfordCars Dataset with around 8000 images in the training set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36adcaba2437d78b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"jutrera/stanford-car-dataset-by-classes-folder\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9acb2da0a98fa38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Investingating the dataset with visualization\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(dataset, num_samples = 20, cols = 4):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for i, img in enumerate(dataset):\n",
    "        if i == num_samples:\n",
    "            break\n",
    "        plt.subplot(num_samples//cols + 1, cols, i+1)\n",
    "        plt.imshow(img[0])\n",
    "root = r\"C:\\Users\\danie\\.cache\\kagglehub\\datasets\\jutrera\\stanford-car-dataset-by-classes-folder\\versions\\2\\car_data\\car_data\"\n",
    "train = torchvision.datasets.ImageFolder(root= rf\"{root}\\train\")\n",
    "test = torchvision.datasets.ImageFolder(root = rf\"{root}\\test\")\n",
    "show_images(train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b70443f699b5826"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Forward Process (Noise Scheduler)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dba2a51fc563c97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end =0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out=vals.gather(-1, t.to(\"cuda\"))\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) -1))).to(\"cuda\")\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=\"cuda\"):\n",
    "    # returns noisy version of an image at certain timestep t\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(sqrt_one_minus_alphas_cumprod, t, x_0.shape)\n",
    "    \n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
    "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
    "\n",
    "# Define beta schedule\n",
    "T = 200\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculated different terms\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis = 0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1,0), value = 1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0/alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "alphas = alphas.to(\"cuda\")\n",
    "betas = betas.to(\"cuda\")\n",
    "alphas_cumprod = alphas_cumprod.to(\"cuda\")\n",
    "alphas_cumprod_prev = alphas_cumprod_prev.to(\"cuda\")\n",
    "sqrt_recip_alphas = sqrt_recip_alphas.to(\"cuda\")\n",
    "sqrt_alphas_cumprod = sqrt_alphas_cumprod.to(\"cuda\")\n",
    "sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.to(\"cuda\")\n",
    "posterior_variance = posterior_variance.to(\"cuda\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87230dca7d1a468c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def load_transformed_dataset(isTraining=True):\n",
    "    data_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), # Scales data into [0,1]\n",
    "        transforms.Lambda(lambda t: (t*2 -1)) # Scale between [-1,1]\n",
    "    ]\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "    \n",
    "    root = r\"C:\\Users\\danie\\.cache\\kagglehub\\datasets\\jutrera\\stanford-car-dataset-by-classes-folder\\versions\\2\\car_data\\car_data\"\n",
    "    train = torchvision.datasets.ImageFolder(root= rf\"{root}\\train\", transform=data_transform)\n",
    "    test = torchvision.datasets.ImageFolder(root = rf\"{root}\\test\", transform=data_transform)\n",
    "    \n",
    "    if isTraining:\n",
    "        return train\n",
    "    else:\n",
    "        return test\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t+1)/2),\n",
    "        transforms.Lambda(lambda t: t.permute(1,2,0)),\n",
    "        transforms.Lambda(lambda t: t * 255),\n",
    "        transforms.Lambda(lambda t: t.byte().cpu().numpy()),\n",
    "        transforms.ToPILImage()\n",
    "    ])\n",
    "    \n",
    "    # Take first image of batch\n",
    "    if len(image.shape)==4:\n",
    "        image = image[0, :, :, :]\n",
    "    plt.imshow(reverse_transforms(image))\n",
    "data = load_transformed_dataset()\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=torch.cuda.is_available(), num_workers=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "336258c03cf984f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Forward Diffusion\n",
    "image = next(iter(dataloader))[0]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.axis('off')\n",
    "num_images = 10\n",
    "stepsize = int(T/num_images)\n",
    "\n",
    "for idx in range(0, T, stepsize):\n",
    "    t = torch.Tensor([idx]).type(torch.int64)\n",
    "    plt.subplot(1, num_images+1, int(idx/stepsize)+1)\n",
    "    image, noise = forward_diffusion_sample(image, t)\n",
    "    show_tensor_image(image)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a130349fdf9d9e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Backward Process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b52719afd7cd000a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding = 1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(3, stride = 2)\n",
    "        self.bnorm = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, t, ):\n",
    "        h = self.bnorm(self.relu(self.conv1(x)))\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        h = h + time_emb\n",
    "        h = self.bnorm(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, time):\n",
    "         device = time.device\n",
    "         half_dim = self.dim // 2\n",
    "         embeddings = math.log(10000) / (half_dim-1)\n",
    "         embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "         embeddings = time[:, None] * embeddings[None, :]\n",
    "         embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "         return embeddings\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    # Simplified variant of the Unet architecture\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_size = IMG_SIZE\n",
    "        image_channels = 3\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        \n",
    "        # Kernel size\n",
    "        out_dim = 1\n",
    "        \n",
    "        # Sinusoidal Embedding length\n",
    "        time_emb_dim = 32\n",
    "        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "        \n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], time_emb_dim) for i in range(len(down_channels)-1)])\n",
    "        \n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], time_emb_dim, up=True) for i in range(len(up_channels)-1)])\n",
    "        \n",
    "        self.output = nn.Conv2d(up_channels[-1], 3, out_dim)\n",
    "        \n",
    "    def forward(self, x, timestep):\n",
    "        t = self.time_mlp(timestep)\n",
    "        x = self.conv0(x)\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "model = SimpleUnet()\n",
    "model.to(\"cuda\")\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45c86c5de95ad83f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loss Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dadd3cbc92396"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t):\n",
    "    x_noisy, noise = forward_diffusion_sample(x_0, t)\n",
    "    noise_pred = model(x_noisy, t)\n",
    "    return F.l1_loss(noise, noise_pred)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f7c2bcfe3facef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sampling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7d4aad56b59ae25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_timestep(x, t):\n",
    "    \"\"\"\n",
    "    Calls model to predict noise in the image and returns the denoised image.\n",
    "    Applies noise to this image, if we are not in the last step yet \n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t)/ sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    \n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "    \n",
    "    if t == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_plot_image():\n",
    "    img_size = IMG_SIZE\n",
    "    img = torch.randn((1, 3, img_size, img_size), device = \"cuda\")\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    num_images = 10\n",
    "    stepsize = int(T/num_images)\n",
    "    \n",
    "    for i in range(0, T)[::-1]:\n",
    "        t = torch.full((1, ), i, device=\"cuda\", dtype=torch.long)\n",
    "        img = sample_timestep(img, t)\n",
    "        if i % stepsize == 0:\n",
    "            plt.subplot(1, num_images, i//stepsize +1)\n",
    "            show_tensor_image(img.detach().to(\"cuda\"))\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b6efbdd9059f297"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54bd58177dc7910f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = Adam(model.parameters(), lr = 0.001)\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        x0 = batch[0].to(\"cuda\" , non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=\"cuda\").long()\n",
    "        loss = get_loss(model, x0, t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step == 0:\n",
    "            print(f\"Epoch {epoch} | step {step:02d} Loss: {loss.item()} \")\n",
    "            sample_plot_image()\n",
    "        print(f\"Epoch: {epoch}, step: {step}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a632efed0aa6683"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch; print(torch.__version__, torch.version.cuda, torch.cuda.is_available())\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fc13b61232b8548"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "IMG_SIZE = (128, 128)   # e.g., (64, 64) or (256, 256)\n",
    "\n",
    "# Resize + ToTensor for PIL images\n",
    "_img_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE, antialias=True),   # force consistent HxW\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def pil_to_tensor_collate(batch):\n",
    "    def convert(sample):\n",
    "        if isinstance(sample, Image.Image):\n",
    "            return _img_transform(sample)\n",
    "\n",
    "        if isinstance(sample, (tuple, list)) and len(sample) > 0:\n",
    "            first = sample[0]\n",
    "            if isinstance(first, Image.Image):\n",
    "                first = _img_transform(first)\n",
    "            return type(sample)([first, *sample[1:]])\n",
    "\n",
    "        if isinstance(sample, dict):\n",
    "            s = dict(sample)\n",
    "            if 'image' in s and isinstance(s['image'], Image.Image):\n",
    "                s['image'] = _img_transform(s['image'])\n",
    "            elif 'x' in s and isinstance(s['x'], Image.Image):\n",
    "                s['x'] = _img_transform(s['x'])\n",
    "            return s\n",
    "\n",
    "        return sample\n",
    "\n",
    "    converted = [convert(s) for s in batch]\n",
    "    return default_collate(converted)\n",
    "\n",
    "\n",
    "# make sure you created test_loader earlier like train_loader\n",
    "test_loader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, pin_memory=torch.cuda.is_available(), collate_fn=pil_to_tensor_collate, num_workers=0)\n",
    "\n",
    "def evaluate_model(model, dataloader, T, device=\"cuda\"):\n",
    "    model.eval()  # set to eval mode\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    print(device)\n",
    "    with torch.no_grad():  # no gradients for evaluation\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            x0 = batch[0].to(device, non_blocking=True)\n",
    "            \n",
    "            # random timesteps for this batch\n",
    "            t = torch.randint(0, T, (x0.size(0),), device=device).long()\n",
    "            \n",
    "            # compute loss (noise prediction error)\n",
    "            loss = get_loss(model, x0, t)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            print(f\"step is {step}\")\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss\n",
    "\n",
    "# Usage after training\n",
    "test_loss = evaluate_model(model, test_loader, T)\n",
    "print(f\"Average test loss: {test_loss:.6f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f48f3f25992c00d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_x0_from_xt(x_t, t, eps_pred):\n",
    "\n",
    "    s1mc_t = get_index_from_list(sqrt_one_minus_alphas_cumprod, t, x_t.shape)  # √(1 - a_bar_t)\n",
    "    sqrt_a_bar_t = get_index_from_list(sqrt_alphas_cumprod, t, x_t.shape)      # √(a_bar_t)\n",
    "    x0_hat = (x_t - s1mc_t * eps_pred) / (sqrt_a_bar_t + 1e-8)\n",
    "    return x0_hat\n",
    "\n",
    "@torch.no_grad()\n",
    "def denoise_batch_examples(model, dataloader, T, device, max_samples):\n",
    "\n",
    "    model.eval()\n",
    "    out = []\n",
    "    taken = 0\n",
    "    for batch in dataloader:\n",
    "        x0 = batch[0].to(device, non_blocking=True)  # clean images in [-1, 1]\n",
    "        B = x0.size(0)\n",
    "\n",
    "        # random t per image\n",
    "        t = torch.randint(0, T, (B,), device=device).long()\n",
    "\n",
    "        # create noisy x_t\n",
    "        x_t, true_noise = forward_diffusion_sample(x0, t)\n",
    "\n",
    "        # predict noise with the model\n",
    "        eps_pred = model(x_t, t)\n",
    "\n",
    "        # estimate x0\n",
    "        x0_hat = estimate_x0_from_xt(x_t, t, eps_pred)\n",
    "\n",
    "        for i in range(B):\n",
    "            out.append((x0[i].detach().cpu(), x_t[i].detach().cpu(), x0_hat[i].detach().cpu(), int(t[i].item())))\n",
    "            taken += 1\n",
    "            if taken >= max_samples:\n",
    "                return out\n",
    "    return out\n",
    "\n",
    "def tensor_to_uint8(image_tensor):\n",
    "    \"\"\"\n",
    "    Convert a single image tensor in [-1,1], shape (C,H,W), to uint8 HWC for PSNR.\n",
    "    \"\"\"\n",
    "    img = (image_tensor.clamp(-1, 1) + 1) / 2.0  # to [0,1]\n",
    "    img = (img * 255.0).round().byte().permute(1, 2, 0).numpy()\n",
    "    return img\n",
    "\n",
    "def compute_metrics(x0, x0_hat):\n",
    "\n",
    "    l1 = F.l1_loss(x0, x0_hat).item()\n",
    "    mse = F.mse_loss(x0, x0_hat).item()\n",
    "\n",
    "    # PSNR computed in [0,255] space\n",
    "    x0_u8     = tensor_to_uint8(x0)\n",
    "    x0_hat_u8 = tensor_to_uint8(x0_hat)\n",
    "    mse_255 = np.mean((x0_u8.astype(np.float32) - x0_hat_u8.astype(np.float32))**2)\n",
    "    if mse_255 <= 1e-10:\n",
    "        psnr = float('inf')\n",
    "    else:\n",
    "        psnr = 10.0 * np.log10((255.0**2) / mse_255)\n",
    "    return l1, mse, psnr\n",
    "\n",
    "def plot_denoise_triplets(triplets):\n",
    "\n",
    "    n = len(triplets)\n",
    "    plt.figure(figsize=(12, 4 * n))\n",
    "    all_l1, all_mse, all_psnr = [], [], []\n",
    "\n",
    "    for i, (x0, x_t, x0_hat, t_int) in enumerate(triplets, start=1):\n",
    "        l1, mse, psnr = compute_metrics(x0, x0_hat)\n",
    "        all_l1.append(l1); all_mse.append(mse); all_psnr.append(psnr)\n",
    "\n",
    "        # Clean\n",
    "        ax = plt.subplot(n, 3, 3*(i-1) + 1)\n",
    "        ax.set_title(f\"Clean x0\")\n",
    "        ax.axis('off')\n",
    "        show_tensor_image(x0.unsqueeze(0))  # your helper supports batch or single\n",
    "\n",
    "        # Noisy\n",
    "        ax = plt.subplot(n, 3, 3*(i-1) + 2)\n",
    "        ax.set_title(f\"Noisy x_t (t={t_int})\")\n",
    "        ax.axis('off')\n",
    "        show_tensor_image(x_t.unsqueeze(0))\n",
    "\n",
    "        # Reconstructed\n",
    "        ax = plt.subplot(n, 3, 3*(i-1) + 3)\n",
    "        ax.set_title(f\"Reconstruction x0_hat\\nL1={l1:.4f}  MSE={mse:.4f}  PSNR={psnr:.2f} dB\")\n",
    "        ax.axis('off')\n",
    "        show_tensor_image(x0_hat.unsqueeze(0))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Print batch averages\n",
    "    print(f\"[Batch metrics]  L1: {np.mean(all_l1):.4f}  MSE: {np.mean(all_mse):.4f}  PSNR: {np.mean(all_psnr):.2f} dB\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_and_visualize(model, test_loader, T, device, max_samples):\n",
    "\n",
    "    triplets = denoise_batch_examples(model, test_loader, T, device, max_samples=max_samples)\n",
    "    plot_denoise_triplets(triplets)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c644c576b1e549b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate_and_visualize(model, test_loader, T, \"cuda\", 10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfe8fe01b7eed5ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3879e62ce78fde42"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
